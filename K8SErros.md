## ğ‘¼ğ’ğ’…ğ’†ğ’“ğ’”ğ’•ğ’‚ğ’ğ’…ğ’Šğ’ğ’ˆ ğ‘²ğ’–ğ’ƒğ’†ğ’“ğ’ğ’†ğ’•ğ’†ğ’” ğ‘·ğ’ğ’… ğ’‡ğ’‚ğ’Šğ’ğ’–ğ’“ğ’†ğ’”

Pods can have startup and runtime errors.

**Startup errors include:**

âœ… ImagePullBackoff 

âœ… ImageInspectError 

âœ… ErrImagePull 

âœ… ErrImageNeverPull 

âœ… RegistryUnavailable 

âœ… InvalidImageName 

**Runtime errors include:**

ğŸ“Œ
âœ… CrashLoopBackOff

âœ… RunContainerError

âœ… KillContainerError

âœ… VerifyNonRootError

âœ… RunInitContainerError

âœ… CreatePodSandboxError

âœ… ConfigPodSandboxError

âœ… KillPodSandboxError

âœ… SetupNetworkError

âœ… TeardownNetworkError

**ğ‘°ğ’ğ’‚ğ’ˆğ’†ğ‘·ğ’–ğ’ğ’ğ‘©ğ’‚ğ’„ğ’Œğ‘¶ğ’‡ğ’‡**

â—
âœ This error appears when #k8s isnâ€™t able to retrieve the image for one of the #containers of the Pod.
There are three common culprits:

âœ… The image name is invalid

âœ… You specified a non-existing tag for the image.

âœ… The image that youâ€™re trying to retrieve belongs to a private registry and the cluster doesnâ€™t have credentials to access it.
The first two cases can be solved by correcting the image name and tag.

For the last, one should add the credentials to your private registry in a Secret and reference it in the Pods

**ğ‘¹ğ’–ğ’ğ‘ªğ’ğ’ğ’•ğ’‚ğ’Šğ’ğ’†ğ’“ğ‘¬ğ’“ğ’“ğ’ğ’“**

â—
âœ The error appears when the container is unable to start before application

**Common causes:**

âœ… Mounting a not-existent volume such as ConfigMap or Secrets

âœ… Mounting a read-only volume as read-write

More detailed aspect can be found by describing the â€˜failedâ€™ pod

**ğ‘ªğ’“ğ’‚ğ’”ğ’‰ğ‘³ğ’ğ’ğ’‘ğ‘©ğ’‚ğ’„ğ’Œğ‘¶ğ’‡ğ’‡**

â—
âœ If the container canâ€™t start, then #Kubernetes shows the CrashLoopBackOff message as a status.
Usually, a container canâ€™t start when:

âœ… Thereâ€™s an error in the application that prevents it from starting.

âœ… You misconfigured the container.

âœ… The Liveness probe failed too many times.


**ğ‘·ğ’ğ’…ğ’” ğ’Šğ’ ğ’‚ ğ‘·ğ’†ğ’ğ’…ğ’Šğ’ğ’ˆ ğ’”ğ’•ğ’‚ğ’•ğ’†**

â—
âœ Assuming that the scheduler component is running fine, here are the causes:

âœ… The cluster doesnâ€™t have enough resources such as CPU and memory to run the Pod.

âœ… The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota.

âœ… The Pod is bound to a Pending PersistentVolumeClaim.

**Note: The best option is to inspect the Events section in the â€œkubectl describeâ€**



# Common Kubernetes Real Time Challenges | Production Scenarios

**1. Effectively Sharing Cluster Resources Among Multiple Development Teams**

**Challenge:** Multiple teams often deploy workloads on the same cluster, leading to resource contention.

**Issues:**

One teamâ€™s application could consume excessive CPU/memory.

Risk of "noisy neighbor" problems.

**Solutions:**

Use Namespaces to logically isolate team environments.

Set Resource Quotas and Limit Ranges to restrict CPU/memory usage per namespace.

Implement Network Policies for traffic isolation.

Monitor usage with Prometheus + Grafana or Kubernetes Metrics Server.

**2. Addressing Out-of-Memory (OOMKilled) Errors in Pods:**

Challenge: Pods are terminated by the kubelet due to exceeding memory limits.

**Causes:**

Inaccurate memory requests/limits.

Memory leaks in applications.

High traffic spikes not accounted for in resource planning.

**Solutions:**

Set appropriate resources.requests and resources.limits in pod specs.

Analyze metrics and logs with kubectl describe pod, Grafana, or ELK Stack.

Conduct load testing and profiling during CI/CD pipeline stages.

Use Horizontal Pod Autoscaler (HPA) to scale pods based on memory or CPU metrics.

**3. Complexities in Performing Kubernetes Cluster Upgrades:**

Challenge: Upgrading Kubernetes without downtime or breaking workloads is complex.

**Issues:**

Compatibility problems with older APIs/deprecated features.

Downtime if upgrades arenâ€™t properly staged.

Risk of cluster instability.

**Solutions:**

Follow a staged upgrade process:

Backup etcd and cluster resources.

Upgrade the control plane first, then worker nodes.

Use kubeadm upgrade or cloud-managed tools (EKS, AKS, GKE upgrade features).

Validate with pre-upgrade testing environments.

Review Kubernetes deprecation notices before upgrades.

Use tools like kured for safe node reboots post-upgrade.


